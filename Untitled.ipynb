{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# workflow V1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) GIS: GIS means Geographic Information System, which mainly includes the following characteristics like (1)public geographic positioning foundation; (2) ability to collect, manage, analyze and output a variety of geospatial information; (3)system driven by analytical model, with strong spatial comprehensive analysis and dynamic prediction capabilities, and can produce high-level geographic information; (4)for the purpose of geographic research and geographic decision-making, it is a human-computer interactive spatial decision support system.\n",
    "\n",
    "b) QGIS: QGIS is a user-friendly, cross-platform, open-source desktop geographic information system based on Qt and developed using C ++.\n",
    "\n",
    "c) GEOTIFF: GeoTIFF based on the TIFF defines a number of the GeoTag (geographic label), to store the various coordinate systems, reference ellipsoid, projection information, so that the number of image data and geographic The data is stored in the same image file .\n",
    "\n",
    "d) Attention model: A resource allocation model. At a certain moment, its attention is always focused on one focal part of the picture, and ignores other parts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to segment the large GEOTIFF picture and send each part of the small picture to the Building Detection model for drawing out buildings, then the detected building group is sent to the Damage level Detection model which can find out how serious the damage is. Finally, we need to union the cut images as 2 GEOTIFF graphs of the building detection result and the damage level detection result. The implementation method is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage I plan to use python's gdal library to cut the huge image. Firstly we read in a set of GEOTIFF pictures (pre and post pictures) as a set of array which save various information, then cut original pictures into 1024 * 1024 pixels small pictures and save them as png format. In order to make sure the house is still complete from cutting, we investigate pixel by pixel at the cutting edge. If the adjacent pixel of the cutting border is also a building and box is not on the far right or top, we move cutting box to the left or down until the building is completely contained in the submap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#read the GeoTIFF file\n",
    "\n",
    "def read_img(self,filename):\n",
    "\n",
    "dataset=gdal.Open(filename) #open it\n",
    "\n",
    "im_width = dataset.RasterXSize\n",
    "\n",
    "im_height = dataset.RasterYSize\n",
    "\n",
    "im_geotrans = dataset.GetGeoTransform() #affine matrix\n",
    "\n",
    "im_proj = dataset.GetProjection() #Map projection information\n",
    "\n",
    "im_data = dataset.ReadAsArray(0,0,im_width,im_height)\n",
    "\n",
    "del dataset\n",
    "\n",
    "return im_proj,im_geotrans,im_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#cut image\n",
    "\n",
    "channel, width, height = data.shape\n",
    "\n",
    "for i in range(width//256):\n",
    "\n",
    "for j in range(height//256):\n",
    "\n",
    "cur_image = data[:,i*256:(i+1)*256,j*256:(j+1)*256]\n",
    "\n",
    "png_image = cur_image.GetDriverByName ('PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the mask RCNN model for building detection. We will compare the trained mask RCNN model with the Microsoft building footprints detection model. We plan to use completely new sample pictures for detection and compare experimental results by calculating values such as accuracy and mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage detection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After detecting the building in the pre image, a pair of single building from pre and post images is cut out and sent to the Damage level Detection model. Here we built a triplet network model. Base model is pre-trained ResNet-50 model, and the building is combined of a 224 * 224 * 6 tensor which will be used to get feature embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picture union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the previous building detection and damage level detection, we generated a corresponding mask for each sub-picture. At this time, we need to merge the pixel information (whether it is a building and its disaster level) of the sub-picture into a new GeoTIFF file with all of the original GEOTIFF information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In future work, we will perform attention model before image segmentation, which can find urban areas for priority segmentation reducing submaps without buildings. Like eyes moving on different objects when a person looks at the image, when a neural network recognizes an image, it focuses on some features at a time, and the recognition is more accurate. The result of the attention model is that at each recognition, the weight of each feature is first calculated, and then the weight summed. The larger the weight, the larger the feature's contribution to the current recognition.\n",
    "\n",
    "exp for attention model 2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
